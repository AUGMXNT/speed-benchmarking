This repo is for benchmarking hardware and software

For results, see:
https://docs.google.com/spreadsheets/d/1kT4or6b0Fedd-W_jMwYpb63e1ZR3aePczz3zlbJW-Y4/edit#gid=1652827441

For some my benchmarking writeups:
- 2025-01-01: [Revisting llama.cpp speculative decoding w/ Qwen2.5-Coder 32B (AMD vs Nvidia results)](https://www.reddit.com/r/LocalLLaMA/comments/1hqlug2/revisting_llamacpp_speculative_decoding_w/)
- 2024-12-17: [Relative performance in llama.cpp when adjusting power limits for an RTX 3090 (w/ scripts)](https://www.reddit.com/r/LocalLLaMA/comments/1hg6qrd/relative_performance_in_llamacpp_when_adjusting/)
- 2024-11-02: [llama.cpp Compute and Memory Bandwidth Efficiency w/ Different Devices/Backends](https://www.reddit.com/r/LocalLLaMA/comments/1ghvwsj/llamacpp_compute_and_memory_bandwidth_efficiency/)
- 2024-11-02: [Testing llama.cpp with Intel's Xe2 iGPU (Core Ultra 7 258V w/ Arc Graphics 140V)](https://www.reddit.com/r/LocalLLaMA/comments/1gheslj/testing_llamacpp_with_intels_xe2_igpu_core_ultra/)
- 2024-10-24: [Tuning for Efficient Inferencing with vLLM on MI300X](https://shisa.ai/posts/tuning-vllm-mi300x/)
- 2024-06-19: [Trainer performance comparison: torchtune vs. axolotl vs. Unsloth](https://wandb.ai/augmxnt/train-bench/reports/torchtune-vs-axolotl-vs-unsloth-Trainer-Performance-Comparison--Vmlldzo4MzU3NTAx)
- 2024-01-09: [AMD Radeon 7900 XT/XTX Inference Performance Comparisons](https://www.reddit.com/r/LocalLLaMA/comments/191srof/amd_radeon_7900_xtxtx_inference_performance/)
