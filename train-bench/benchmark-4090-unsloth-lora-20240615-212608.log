Sat Jun 15 09:26:09 PM JST 2024
===
OS: Arch Linux (6.9.3-arch1-1)
CPU: AMD Ryzen 9 5950X 16-Core Processor (16C 32T @ 5.0 GHz)
RAM: 62.7 GiB
Nvidia GPUs:
  NVIDIA GeForce RTX 3090 (24.0 GiB)
  NVIDIA GeForce RTX 4090 (24.0 GiB)
CUDA Version: 12.3, V12.3.107
PyTorch: 2.3.0+cu121
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
==((====))==  Unsloth: Fast Llama patching release 2024.6
   \\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.643 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
{'loss': 1.4349, 'grad_norm': 0.6602087020874023, 'learning_rate': 5e-06, 'epoch': 0.0}
{'loss': 1.3084, 'grad_norm': 0.5930176973342896, 'learning_rate': 1e-05, 'epoch': 0.0}
{'loss': 1.4408, 'grad_norm': 0.6909933090209961, 'learning_rate': 1.5e-05, 'epoch': 0.01}
{'loss': 1.423, 'grad_norm': 0.6907145977020264, 'learning_rate': 2e-05, 'epoch': 0.01}
{'loss': 1.2722, 'grad_norm': 0.5735419392585754, 'learning_rate': 2.5e-05, 'epoch': 0.01}
{'loss': 1.476, 'grad_norm': 0.838977038860321, 'learning_rate': 3e-05, 'epoch': 0.01}
{'loss': 1.2345, 'grad_norm': 0.6296031475067139, 'learning_rate': 3.5e-05, 'epoch': 0.02}
{'loss': 1.2934, 'grad_norm': 0.7119511365890503, 'learning_rate': 4e-05, 'epoch': 0.02}
{'loss': 1.311, 'grad_norm': 0.8065990805625916, 'learning_rate': 4.5e-05, 'epoch': 0.02}
{'loss': 1.3298, 'grad_norm': 0.7366756200790405, 'learning_rate': 5e-05, 'epoch': 0.02}
{'loss': 1.3222, 'grad_norm': 0.6979731321334839, 'learning_rate': 4.9e-05, 'epoch': 0.03}
{'loss': 1.2743, 'grad_norm': 0.5263401865959167, 'learning_rate': 4.8e-05, 'epoch': 0.03}
{'loss': 1.2488, 'grad_norm': 0.4311552345752716, 'learning_rate': 4.7e-05, 'epoch': 0.03}
{'loss': 1.2485, 'grad_norm': 0.47027453780174255, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.03}
{'loss': 1.2411, 'grad_norm': 0.6516779065132141, 'learning_rate': 4.5e-05, 'epoch': 0.04}
{'loss': 1.2063, 'grad_norm': 0.5092297792434692, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.04}
{'loss': 1.2712, 'grad_norm': 0.42007216811180115, 'learning_rate': 4.3e-05, 'epoch': 0.04}
{'loss': 1.2134, 'grad_norm': 0.38307127356529236, 'learning_rate': 4.2e-05, 'epoch': 0.04}
{'loss': 1.1729, 'grad_norm': 0.3858226537704468, 'learning_rate': 4.1e-05, 'epoch': 0.05}
{'loss': 1.259, 'grad_norm': 0.3709006905555725, 'learning_rate': 4e-05, 'epoch': 0.05}
{'loss': 1.2246, 'grad_norm': 0.3098733723163605, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.05}
{'loss': 1.1341, 'grad_norm': 0.2725845277309418, 'learning_rate': 3.8e-05, 'epoch': 0.05}
{'loss': 1.1625, 'grad_norm': 0.2959148585796356, 'learning_rate': 3.7e-05, 'epoch': 0.06}
{'loss': 1.1843, 'grad_norm': 0.2690432369709015, 'learning_rate': 3.6e-05, 'epoch': 0.06}
{'loss': 1.2443, 'grad_norm': 0.24349293112754822, 'learning_rate': 3.5e-05, 'epoch': 0.06}
{'loss': 1.2164, 'grad_norm': 0.2907056510448456, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.06}
{'loss': 1.1444, 'grad_norm': 0.2869237959384918, 'learning_rate': 3.3e-05, 'epoch': 0.07}
{'loss': 1.1995, 'grad_norm': 0.26747092604637146, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.07}
{'loss': 1.1515, 'grad_norm': 0.26368066668510437, 'learning_rate': 3.1e-05, 'epoch': 0.07}
{'loss': 1.249, 'grad_norm': 0.30330899357795715, 'learning_rate': 3e-05, 'epoch': 0.07}
{'loss': 1.2698, 'grad_norm': 0.3621142506599426, 'learning_rate': 2.9e-05, 'epoch': 0.08}
{'loss': 1.2255, 'grad_norm': 0.3001040518283844, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.08}
{'loss': 1.2328, 'grad_norm': 0.2997390031814575, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.08}
{'loss': 1.233, 'grad_norm': 0.2732243239879608, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.08}
{'loss': 1.1241, 'grad_norm': 0.22584402561187744, 'learning_rate': 2.5e-05, 'epoch': 0.09}
{'loss': 1.1699, 'grad_norm': 0.2821124792098999, 'learning_rate': 2.4e-05, 'epoch': 0.09}
{'loss': 1.175, 'grad_norm': 0.5250453352928162, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.09}
{'loss': 1.1978, 'grad_norm': 0.3061559200286865, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.09}
{'loss': 1.2278, 'grad_norm': 0.27025526762008667, 'learning_rate': 2.1e-05, 'epoch': 0.1}
{'loss': 1.1167, 'grad_norm': 0.22039777040481567, 'learning_rate': 2e-05, 'epoch': 0.1}
{'loss': 1.2362, 'grad_norm': 0.2705708146095276, 'learning_rate': 1.9e-05, 'epoch': 0.1}
{'loss': 1.1941, 'grad_norm': 0.28783562779426575, 'learning_rate': 1.8e-05, 'epoch': 0.1}
{'loss': 1.1594, 'grad_norm': 0.22601313889026642, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.11}
{'loss': 1.1362, 'grad_norm': 0.2923875153064728, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.11}
{'loss': 1.153, 'grad_norm': 0.2710511386394501, 'learning_rate': 1.5e-05, 'epoch': 0.11}
{'loss': 1.2198, 'grad_norm': 0.30941957235336304, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.11}
{'loss': 1.1289, 'grad_norm': 0.24333974719047546, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.12}
{'loss': 1.1904, 'grad_norm': 0.2564637064933777, 'learning_rate': 1.2e-05, 'epoch': 0.12}
{'loss': 1.1134, 'grad_norm': 0.24720463156700134, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.12}
{'loss': 1.1764, 'grad_norm': 0.26565831899642944, 'learning_rate': 1e-05, 'epoch': 0.12}
{'loss': 1.1453, 'grad_norm': 0.23289035260677338, 'learning_rate': 9e-06, 'epoch': 0.13}
{'loss': 1.0988, 'grad_norm': 0.20922356843948364, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.13}
{'loss': 1.2258, 'grad_norm': 0.3109060227870941, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.13}
{'loss': 1.2206, 'grad_norm': 0.28046804666519165, 'learning_rate': 6e-06, 'epoch': 0.13}
{'loss': 1.2114, 'grad_norm': 0.2843089699745178, 'learning_rate': 5e-06, 'epoch': 0.14}
{'loss': 1.1351, 'grad_norm': 0.2727832496166229, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.14}
{'loss': 1.1206, 'grad_norm': 0.20269577205181122, 'learning_rate': 3e-06, 'epoch': 0.14}
{'loss': 1.1733, 'grad_norm': 0.2526976764202118, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.14}
{'loss': 1.1047, 'grad_norm': 0.2811295688152313, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.15}
{'loss': 1.2327, 'grad_norm': 0.3559756278991699, 'learning_rate': 0.0, 'epoch': 0.15}
{'train_runtime': 894.9159, 'train_samples_per_second': 8.582, 'train_steps_per_second': 0.067, 'train_loss': 1.2206844846407572, 'epoch': 0.15}
